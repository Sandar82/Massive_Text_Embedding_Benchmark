{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tr_jEBnh-jv"
      },
      "source": [
        "# MTEB: Massive Text Embedding Benchmark:\n",
        "\n",
        "#### Group Member Names : Sandar Aung & Maria Namitha Nelson\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeKSxMvrh-j0"
      },
      "source": [
        "### INTRODUCTION:\n",
        "\n",
        "<p align=\"justify\">\n",
        "The document introduces the Massive Text Embedding Benchmark (MTEB), which addresses the challenge of evaluating text embeddings across various tasks and datasets. The authors highlight that current text embedding evaluations are often limited to specific tasks, such as semantic textual similarity (STS), and do not cover a broader range of applications like clustering or reranking. MTEB spans eight embedding tasks, covering 58 datasets in 112 languages, and provides a comprehensive benchmark to assess the performance of 33 models. The findings suggest that no single text embedding method excels across all tasks, indicating the need for further research to develop a universal text embedding model. MTEB is designed to simplify the evaluation of text embeddings and comes with open-source code and a public leaderboard </p>\n",
        "\n",
        "#### AIM :\n",
        "<p align=\"justify\">\n",
        "The aim of the Massive Text Embedding Benchmark (MTEB) is to provide a comprehensive evaluation framework that assesses the performance of text embedding models across a wide range of tasks and datasets. By encompassing eight different embedding tasks and including 58 datasets in 112 languages, MTEB seeks to address the limitations of existing evaluation regimes that often focus on a narrow set of tasks. The benchmark is designed to help identify which models are best suited for various use cases, ultimately guiding the development of more universally effective text embedding methods </p>\n",
        "*********************************************************************************************************************\n",
        "#### Github Repo:\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### DESCRIPTION OF PAPER:\n",
        "<p align=\"justify\">\n",
        "The paper describes the Massive Text Embedding Benchmark (MTEB), which is designed to evaluate the performance of text embedding models across various tasks and datasets. MTEB covers eight different task types, including classification, clustering, and retrieval, utilizing 58 datasets in 112 languages. The benchmark provides a comprehensive evaluation framework, helping researchers and practitioners assess and compare the effectiveness of different embedding models. The paper also highlights the extensibility of MTEB, allowing for the inclusion of new datasets and tasks, and discusses the limitations and future directions for the benchmark. </p>\n",
        "*********************************************************************************************************************\n",
        "#### PROBLEM STATEMENT :\n",
        "<p align=\"justify\">\n",
        "The problem statement identified in the document is that current text embedding models are often evaluated on a limited set of tasks, primarily focusing on semantic textual similarity (STS), which does not adequately represent the broad range of real-world applications such as clustering, reranking, or other text processing tasks. This narrow evaluation approach hinders the ability to track progress in the field and leads to the \"blind\" application of these models to new use cases without proper evaluation, resulting in potential inefficiencies and inaccuracies in their deployment.</p>\n",
        "*********************************************************************************************************************\n",
        "#### CONTEXT OF THE PROBLEM:\n",
        "*\n",
        "\n",
        "<p align=\"justify\">\n",
        "The context of the problem addressed in the document is that text embedding models are typically evaluated on a narrow set of tasks, such as semantic textual similarity (STS), which does not fully capture their applicability to a broader range of real-world tasks like clustering, retrieval, or reranking. This limited evaluation scope makes it challenging to track progress in the field and leads to the \"blind\" application of these models to new tasks without proper validation, which could result in suboptimal performance in practical applications.</p>\n",
        "*********************************************************************************************************************\n",
        "#### SOLUTION:\n",
        "*\n",
        "<p align=\"justify\">\n",
        "The solution proposed in the paper is the development of the Massive Text Embedding Benchmark (MTEB), which provides a unified, accessible evaluation framework that consolidates datasets from various embedding tasks. MTEB incorporates a wide range of tasks and datasets, including popular benchmarks like SemEval and BEIR, to offer a comprehensive review of text embedding models' performance. By providing an open-source, extensible platform, MTEB allows researchers to evaluate any embedding model with minimal setup, fostering better understanding and development of universal text embeddings that can be applied across diverse tasks. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77PIPLQ-h-j1"
      },
      "source": [
        "# Background\n",
        "\n",
        "## Reference:\n",
        "<p align=\"justify\">\n",
        "The paper references various embedding tasks, such as semantic textual similarity (STS) and bitext mining, incorporating datasets like SemEval and BEIR into a unified evaluation framework for text embedding models. </p>\n",
        "\n",
        "## Explanation:\n",
        "<p align=\"justify\">\n",
        "The MTEB benchmark aims to address the limitations of current evaluation methods by providing a more comprehensive and accessible framework. It allows for the assessment of embedding models across multiple tasks, offering a more holistic view of their performance. </p>\n",
        "\n",
        "## Dataset/Input:\n",
        "<p align=\"justify\">\n",
        "MTEB includes 58 datasets covering 8 different tasks, including classification, clustering, retrieval, and summarization. These datasets span 112 languages and include both sentence-level and paragraph-level texts. </p>\n",
        "\n",
        "## Weakness:\n",
        "<p align=\"justify\">\n",
        "The benchmark faces certain limitations, such as a lack of long document datasets and an imbalance in the number of datasets across different tasks, which may skew the overall evaluation results. Additionally, it currently lacks comprehensive support for multilingual retrieval and code datasets.</p>\n",
        "*********************************************************************************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deODH3tMh-j2"
      },
      "source": [
        "# Implement paper code :\n",
        "*********************************************************************************************************************\n",
        "\n",
        "*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gkHhku9h-j2"
      },
      "source": [
        "*********************************************************************************************************************\n",
        "### Contribution  Code :\n",
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YdFCgWoh-j3"
      },
      "source": [
        "### Results :\n",
        "*******************************************************************************************************************************\n",
        "\n",
        "\n",
        "#### Observations :\n",
        "*******************************************************************************************************************************\n",
        "*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3JVj9dKh-j3"
      },
      "source": [
        "### Conclusion and Future Direction :\n",
        "\n",
        "#### Conclusion\n",
        "<p align=\"justify\">\n",
        "The paper presents the Massive Text Embedding Benchmark (MTEB), which provides a comprehensive evaluation framework for text embedding models across various tasks and datasets. Through extensive benchmarking involving nearly 5,000 experiments on over 30 models, the study establishes solid baselines and reveals significant performance variability among models across different tasks. The results indicate that no single model excels universally, highlighting the importance of task-specific model selection. MTEB is designed to be extensible and encourages community contributions to further enhance the benchmark. </p>\n",
        "\n",
        "#### Future Direction\n",
        "<p align=\"justify\">\n",
        "Future directions include expanding MTEB to cover additional tasks, datasets, and modalities, such as long document datasets, multilingual retrieval, and code datasets. The paper also suggests incorporating benchmarks for text embeddings used as inputs for other modalities like images. The authors invite contributions from the research community to continuously improve the MTEB framework and maintain its relevance in evaluating text embedding models .\n",
        "</p>\n",
        "\n",
        "#### Learnings :\n",
        "\n",
        "<p align=\"justify\">\n",
        "The paper demonstrates that no single text embedding model excels across all tasks, highlighting the importance of task-specific model selection. The benchmarking results reveal significant variability in model performance across different tasks and datasets. Larger models tend to dominate in performance, but they come with higher computational costs. The research underscores the need for comprehensive and diverse evaluation frameworks, like MTEB, to accurately assess and guide the development of text embedding models for various applications .</p>\n",
        "\n",
        "#### Results Discussion :\n",
        "<p align=\"justify\"> </p>\n",
        "\n",
        "*******************************************************************************************************************************\n",
        "\n",
        "#### Limitations :\n",
        "\n",
        "<p align=\"justify\">\n",
        "Long Document Datasets: MTEB includes multiple text lengths, but it lacks very long document datasets, which are essential for certain use cases like retrieval. The longest datasets in MTEB only contain a few hundred words, which limits the evaluation of models designed for processing longer texts.</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Task Imbalance: There is an imbalance in the number of datasets across different tasks within MTEB. For example, summarization consists of only a single dataset, leading to a potential bias in average scores towards tasks with more datasets, such as retrieval, classification, and clustering.</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Multilinguality: While MTEB includes multilingual datasets for classification, STS, and bitext mining, it lacks multilingual retrieval and clustering datasets. Additionally, it does not include any code datasets, limiting its ability to benchmark code models comprehensively.</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Additional Modalities:The benchmark focuses solely on natural language applications and does not extensively benchmark text embeddings used as input features for other modalities, such as image content. This limits the evaluation scope for models that integrate multiple types of data . </p>\n",
        "\n",
        "\n",
        "*******************************************************************************************************************************\n",
        "#### Future Extension :\n",
        "<p align=\"justify\">\n",
        "Expansion of Datasets: The benchmark can be extended to include long document datasets, more tasks like summarization and pair classification, and additional multilingual retrieval datasets. This will address the current task imbalance and provide a more comprehensive evaluation framework.</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Inclusion of Code Datasets: MTEB could be expanded to evaluate text embedding models on code datasets, which are currently not covered. This would involve incorporating datasets like CodeSearchNet, TyDI QA, XOR QA, and MIRACL, enhancing the benchmark's ability to evaluate models used in code-related tasks.</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Additional Modalities: Future work could involve benchmarking text embeddings as input features for other modalities, such as image content. This would broaden the scope of MTEB beyond natural language applications and make it more versatile for evaluating models used in multimodal scenarios  </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATXtFdtBh-j4"
      },
      "source": [
        "# References:\n",
        "\n",
        "[1]:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQnMSAf-h-j4"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}